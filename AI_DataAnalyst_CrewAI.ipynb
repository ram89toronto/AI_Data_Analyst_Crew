{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fmt0Da7hzRzd",
        "outputId": "38776f35-f80f-4157-9096-cfbf24ce1238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/bcrypt/\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m754.4/754.4 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m552.7/552.7 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m128.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.5/155.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m119.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.1/250.1 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.0/352.0 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.7/38.7 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.1/485.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.6/229.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip -q install -U crewai[tools] pandas matplotlib python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, textwrap, shutil, sys, pathlib, datetime\n",
        "from pprint import pprint\n",
        "\n",
        "#Configuration API Keys\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"Enter Your Key Here\"\n",
        "assert \"OPENAI_API_KEY\" in os.environ, \"OPENAI_API_KEY is not available, please enter your own api key \"\n",
        "\n",
        "# basic paths\n",
        "\n",
        "BASE_DIR = pathlib.Path().cwd()\n",
        "OUT_DIR = BASE_DIR / \"out\"\n",
        "FIG_DIR = OUT_DIR / \"figs\"\n",
        "OUT_DIR.mkdir(exist_ok=True)\n",
        "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\" Environment OK. Output Dir:\", OUT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpdmjxuP1Ijd",
        "outputId": "6430beb3-f4fb-4327-b626-31d08b9f3ecd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Environment OK. Output Dir: /content/out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary tools and create directory for tools\n",
        "\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from crewai_tools import CodeInterpreterTool\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "_2lhzJ5Y6A3Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating necessary files in the subfolders and files\n",
        "\n",
        "def write_text(path: pathlib.Path, text: str):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    path.write_text(text, encoding=\"utf-8\")\n",
        "\n",
        "def read_text(path: pathlib.Path) -> str:\n",
        "    return path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "def reset_out():\n",
        "  if OUT_DIR.exists():\n",
        "    shutil.rmtree(OUT_DIR)\n",
        "  OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "  FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "bZWBdXFi6yRG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReAct Prompts\n",
        "PROMPT_MANAGER = \"\"\"\n",
        "You are the Analytics Manager. You job is to plan, delegate, verify outputs, and request fixes.\n",
        "Process:\n",
        "1) Review the goal and inputs (csv_path, figdir, report_path)\n",
        "2) Delegate to ingestion, the eda, then viz, then insights.\n",
        "3) For each step, if expected files are missing or malformed, send a concise correction and retry once.\n",
        "4) Keep everything deterministic and minimal. No external network calls. Keep plots Matplotlib-only.\n",
        "\n",
        "Deliverable: a successful run with:\n",
        "- out/df_cleaned.pkl\n",
        "- out/schema.json\n",
        "- out/eda_summary.json\n",
        "- out/figs_index.json and PNGs under {figdir}\n",
        "- {report_path} (markdown with insights & next actions)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "PROMPT_INGESTION = \"\"\"\n",
        "Role: Data Ingestion & Cleaning Engineer.\n",
        "\n",
        "Goal: Load the CSV at `csv_path`, infer schema, clean NA/outliers, and persist artifacts.\n",
        "\n",
        "Constraints and Tools:\n",
        "- Use Python (pandas). Write short, correct code via the code interpreter tool.\n",
        "- Never use seaborn; do not produce plots here.\n",
        "- Be explicit and reproducible: if you choerce types, log it to a 'changlog'.\n",
        "\n",
        "Steps (ReAct):\n",
        "- REASON: Inspect the CSV path and file size. Determine read_csv parameters (parse_dates candidates, dtype hints).\n",
        "- ACT (code): Read with pandas (low_memory=False). Print df.info(). sample(5).\n",
        "- OBSERVE: If bad dtypes or parse errors, re-load with better params.\n",
        "- ADJUST: Apply cleaning : strip column names, drop exact duplicates, make best-effort type coercions.\n",
        "- For NAs: decide column-wise strategy (drop/median/most_frequent/leave) and justify.\n",
        "- Flag obvious outliers (z-score > 4 on numeric columns) into a boolean mask column `__outlier_flag` (do NOT drop).\n",
        "\n",
        "Deliverables:\n",
        "- Save cleaned DataFrame to 'out/df_cleaned.pkl'\n",
        "- Write to 'out/schema.json' capturing: columns, inferred dtypes, NA counts, choosen NA strategies, any coercions, and a short 'changelog'\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "PROMPT_EDA = \"\"\"\n",
        "Role: EDA Specialist.\n",
        "\n",
        "Goal: Analyze 'out/df_cleaned.pkl' and produce 'out/eda_summary.json'.\n",
        "\n",
        "Constraints:\n",
        "- Python +pandas only. No Seaborn. No external data.\n",
        "- Be Selective: Surface only useful, interpretable stats.\n",
        "\n",
        "Steps (ReAct):\n",
        "- REASON: Identify numeric vs categorical columns. Detect datetime columns. Check cardinality\n",
        "- ACT (code): Compute:\n",
        "   - Descriptive Stats (numeric describe, categorical top frequencies)\n",
        "   - Correlations (Pearson); if time column exists, simple trand stats\n",
        "   - Missingness summary (post-clean)\n",
        "- OBSERVE: Extract key findings (e.g., skewed distribution, strong correlations |r| >= 0.5).\n",
        "- ADJUST: Propose 3-6 'candidate_plots' with {type, columns, rationale}.\n",
        "\n",
        "Deliverable JSON ('out/eda_summary.json'):\n",
        " {\n",
        "   \"highlights\": [string...],\n",
        "   \"candidate_plots\": [\n",
        "     {\n",
        "       {\"type\": \"hist\", \"columns\": [\"col\"], \"rationale\": \"...\"},\n",
        "       {\"type\": \"bar\", \"columns\": [\"cat_col\"], \"rationale\": \"...\"},\n",
        "       {\"type\": \"line\", \"columns\": [\"time_col\",\"metric\"], \"rationale\": \"...\"},\n",
        "       ....\n",
        "]\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_VIZ = \"\"\"\n",
        "Role: Visualization Engineer.\n",
        "\n",
        "\n",
        "Goal: Generate Matplotlib PNGs for the top candidate plots and save an index.\n",
        "\n",
        "Constraints:\n",
        "  - Use Matplotlib ONLY. One Figure per plot. save under figdir as PNG.\n",
        "  - No style settings, no custom colors\n",
        "  - Handle missing columns gracefully (skip with a note)\n",
        "\n",
        "\n",
        "Plot rules:\n",
        "  - hist: df[columns[0]].hist()\n",
        "  - bar: df[columns[0]].value_counts().head(20).plot(kind='bar')\n",
        "  - line: If len(columns)==2 and first is datetime-like, plot df.sort_values(time)[[time, metric]] as a line.\n",
        "  - scatter: If 2 numeric columns, simple scatter.\n",
        "  - box: df[columns].plot(kind=\"box\")\n",
        "\n",
        "Deliverable:\n",
        "  - Save 3-6 PNGs named fig_01.png, fig_02.png, .....\n",
        "  - Write 'out/figs_index.json' as a list: [{\"file\"}:\"out/fig_01.png\", \"caption\":\"...\"},...]\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_INSIGHTS = \"\"\"\n",
        "Role: Insights & Strategy Analyst.\n",
        "\n",
        "Goal: Produce a crisp analyst report 'report_path' in Markdown using:\n",
        "  - out/schema.json\n",
        "  - out/eda_summary.json\n",
        "  - out/figs_index.json\n",
        "\n",
        "Report Sections:\n",
        "#Title\n",
        "-Dataset Overview (rows, columns, key types)\n",
        "-Data quality summary (NA handling, outlier flags)\n",
        "-Top 3-5 insights with specific stats / correlations\n",
        "-Visuals: embed filenames and explain what to look at\n",
        "-Decisions & Next Actions: concrete recommendations (e.g., product , marketing, ops, service logs)\n",
        "-Risks & Assumptions\n",
        "-Appendix: table of columns with short notes\n",
        "\n",
        "Style:\n",
        "-Be consise, Use Bullet points.\n",
        "-Quote numbers precisely (means, medians, correlations)\n",
        "-Refer to figures by filename. Do NOT try to display them; just list paths.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gb4rn3i3MgDG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Agents & Tasks\n",
        "\n",
        "code_tool= CodeInterpreterTool()\n",
        "\n"
      ],
      "metadata": {
        "id": "Xhwlo5sTGRRk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "manager = Agent( role = \"Analytics Manager\",\n",
        "                 goal =\"Deliver an accurate, visual, decision-oriented analysis from a CSV path.\",\n",
        "                 backstory=\" A senior data lead who plans work, checks results, and requests fixes.\",\n",
        "                 allow_delegation=True,\n",
        "                 verbose=True,\n",
        "                 llm='gpt-5',\n",
        "                 memory=True,\n",
        "                 max_iter=2,\n",
        "                 system_prompt=PROMPT_MANAGER,\n",
        ")\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "rR-JIz7zK2hi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ingestion = Agent(\n",
        "    role=\"Data Ingestion and Cleaning\",\n",
        "    goal = \"Load CSV, infer schema, clean data robustly, persist df_cleaned.pkl + schema JSON.\",\n",
        "    backstory =\" A senior data lead who plans work, checks results, and request fixex.\",\n",
        "    allow_delegation=False,\n",
        "    verbose=True,\n",
        "    llm=\"gpt-5\",\n",
        "    tools=[code_tool],\n",
        "    max_iter=3,\n",
        "    system_prompt=PROMPT_INGESTION\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jpGJ8VVKLpRi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eda = Agent(\n",
        "    role=\"EDA Specialist\",\n",
        "    goal = \"Summaries, stats, correlations; propose meaningful visulalizations\",\n",
        "    backstory =\"Quant who spots patterns and pitfalls.\",\n",
        "    allow_delegation=False,\n",
        "    verbose=True,\n",
        "    llm=\"gpt-5\",\n",
        "    tools=[code_tool],\n",
        "    max_iter=3,\n",
        "    system_prompt=PROMPT_EDA\n",
        ")\n"
      ],
      "metadata": {
        "id": "MFOSJ-PKOWrs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viz = Agent(\n",
        "    role=\"Vizualization Engineer\",\n",
        "    goal = \"Produce clear Matlotlib PNGs for findings; save and list captions.\",\n",
        "    backstory =\" Programatic plotter with clean defaults\",\n",
        "    allow_delegation=False,\n",
        "    verbose=True,\n",
        "    llm=\"gpt-5\",\n",
        "    tools=[code_tool],\n",
        "    max_iter=3,\n",
        "    system_prompt=PROMPT_VIZ\n",
        ")\n"
      ],
      "metadata": {
        "id": "uNd4aYJKPSPC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insights = Agent(\n",
        "    role=\"Insights & Strategy\",\n",
        "    goal = \"Draft a crisp report with insights and next-step recommendations\",\n",
        "    backstory =\" BI analyst who writes for execs.\",\n",
        "    allow_delegation=False,\n",
        "    verbose=True,\n",
        "    llm=\"gpt-5\",\n",
        "    tools=[code_tool],\n",
        "    max_iter=3,\n",
        "    system_prompt=PROMPT_INSIGHTS\n",
        ")\n"
      ],
      "metadata": {
        "id": "xT_i2MgqP8w4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tasks\n",
        "\n",
        "def make_tasks(csv_path, figdir:str, report_path:str):\n",
        "\n",
        "  t1= Task(\n",
        "      description=textwrap.dedent(f\"\"\" Ingest {csv_path}. Infer dtypes, diagnose NAs/outliers, clean with transparent steps. Save DataFrame at 'out/df_cleaned.pkl'\n",
        "      and write 'out/schema.json' describing columns and decisions. Print a short changelog.\"\"\").strip(),\n",
        "      agent=ingestion,\n",
        "      expected_output=\"out/df_cleaned.pkl and out/schema.json created\",\n",
        "  )\n",
        "\n",
        "  t2= Task(\n",
        "      description=textwrap.dedent(f\"\"\" Load 'out/df_cleaned.pkl. Compute descriptive stats, correlations, key categorical distributions, and generate a ranked list of 'candidate_plots' with (type, columns, rationale). Save at 'out/eda_summary.json'\n",
        "      \"\"\").strip(),\n",
        "      agent=eda,\n",
        "      expected_output=\"out/eda_summary.json created with candidate_plots list\",\n",
        "  )\n",
        "\n",
        "  t3= Task(\n",
        "      description=textwrap.dedent(f\"\"\" Read 'out/eda_summary.json'. For the top 3-6 candiate plots, generate Matplotlib figures and save PNGs to {figdir} and Avoid Seaborn\n",
        "      . Return a JSON array of {{file, caption}} and write it to 'out/figs_index.json'. \"\"\").strip(),\n",
        "      agent=viz,\n",
        "      expected_output=f\"PNG figures saved under {figdir} and 'out/figs_index.json' created\",\n",
        "  )\n",
        "\n",
        "  t4= Task(\n",
        "      description=textwrap.dedent(f\"\"\" Using  'out/schema.json', 'out/eda_summary.json' and 'out/figs_index.json', write a Markdown reprot to {report_path}. The\n",
        "      report must include sections: Overview, data quality, insights, Visuals {filenames}, Decisions & Next Actions: concrete recommendations (e.g., product , marketing, ops, service logs)\n",
        "-Risks & Assumptions\n",
        "-Appendix: table of columns with short notes\n",
        "\n",
        "  \"\"\").strip(),\n",
        "      agent=insights,\n",
        "      expected_output=f\"{report_path} written\",\n",
        "  )\n",
        "\n",
        "  return [t1,t2, t3, t4]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N_FrSj2cQbk3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the crew Hirarchial setup for the manager can verify and redirect\n",
        "\n",
        "def make_crew(tasks):\n",
        "\n",
        "  crew = Crew(\n",
        "\n",
        "              agents= [ingestion, eda, viz, insights],\n",
        "              manager_agent= manager,\n",
        "              tasks=tasks,\n",
        "              verbose=True,\n",
        "              max_rpm=30,\n",
        "              process=Process.hierarchical\n",
        "  )\n",
        "  return crew\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r-cpukKFXkKr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Runner: End to End pipeline\n",
        "\n",
        "def run_analytics(csv_path:str, figdir:str = str(FIG_DIR), report_path:str = str(OUT_DIR/ \"report.md\"), reset_outputs:bool = True):\n",
        "\n",
        "  if reset_outputs:\n",
        "    reset_out()\n",
        "\n",
        "\n",
        "    assert pathlib.Path(csv_path).exists(), f\"CSV not found: {csv_path}\"\n",
        "    write_text(OUT_DIR / \"RUN_METADATA.json\", json.dumps({\n",
        "        \"csv_path\": str(csv_path),\n",
        "        \"figdir\": str(figdir),\n",
        "        \"report_path\": str(report_path),\n",
        "        \"run_at\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
        "\n",
        "\n",
        "\n",
        "    }, indent =2))\n",
        "\n",
        "    tasks = make_tasks(csv_path=csv_path, figdir=figdir, report_path=report_path)\n",
        "    crew= make_crew(tasks)\n",
        "    result = crew.kickoff()\n",
        "    print(\"\\n=== Crew Result ===== \\n\", result)\n",
        "\n",
        "    # Acceptance Checks\n",
        "\n",
        "    required = [\n",
        "        OUT_DIR / \"df_cleaned.pkl\",\n",
        "        OUT_DIR / \"schema.json\",\n",
        "        OUT_DIR / \"eda_summary.json\",\n",
        "        OUT_DIR / \"figs_index.json\",\n",
        "        pathlib.Path(report_path),\n",
        "    ]\n",
        "\n",
        "    missing = [str(p) for p in required if not pathlib.Path(p).exists()]\n",
        "\n",
        "    if missing:\n",
        "      raise RuntimeError(\"Missing expected outputs: \\n \" + \"\\n\".join(missing))\n",
        "\n",
        "    #Show a quick summary\n",
        "\n",
        "    print(\"\\n Artifacts:\")\n",
        "\n",
        "    for p in required:\n",
        "      print(f\" - {p}\")\n",
        "\n",
        "    with open(OUT_DIR / \"figs_index.json\",\"r\", encoding=\"utf-8\") as f:\n",
        "      figs_index = json.load(f)\n",
        "\n",
        "    print(\"\\n Figures:\")\n",
        "\n",
        "    for i in idx:\n",
        "      print(\" - \", i.get(\"file\"), \"::\". i.get(\"caption\"))\n",
        "\n",
        "    print(f\"\\n Done. Report : {report_path}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P6bUn1FsZgpf"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}